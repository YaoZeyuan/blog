---
title: ChatGPT 资料梳理
tags: 人工智能
date: 2023-02-20 13:00:00
---

> 梳理 ChatGPT 的相关资料

# 非技术视角看 ChatGPT

## [最近很火的 ChatGPT 究竟是什么？会给我们的生活带来什么改变？](https://mp.weixin.qq.com/s/GkUPpftkv5fS6qHMZm8ZuA)

- 背景
  - 基于非技术视角对 ChatGPT 的总结
- 核心观点
  - chatGPT 能做什么
    - 回答问题
    - 撰写文章
    - 总结提炼
    - 生成代码
  - chatGPT 劣势
    - 错误率高, 且难以(内行)/无法(外行)分辨正确性
    - 价值观可能有问题(对公司内部应用, 暂不考虑)
  - 应用前景
    - 搜索
      - 基于搜索引擎的搜索结果进行整合, 给出更适合查阅的结果
      - (注意, 基于成本考虑, 该功能很难免费提供)
    - 阅读/写作
    - 训练私有智能助手

# 业务/应用场景分析

## [潘一鸣: AIGC 风口，人工智能又又又行了吗](https://mp.weixin.qq.com/s/HrFrUgrDyGRYphwbgpHvvg)

- 核心观点:
- 爆火的原因: 用户感知到，AI 生产的内容，已经在某些达到了以假乱真的地步，甚至在有些场景下好像超过了人类.
  - 产生误解: 原本以为 AI 无法攻克的文本创作和图片创作领域好像被突破了
- 目前的 AI 性能提升，并不是算法的底层突破（应用层还是有创新的)，而是受益于大数据和云计算的发展.
  - 联结学派可以处理海量数据，在合适的算法结构下，能解决多方面的问题. 强化学习在算力增强的情况下，性能可以持续提升
  - 有人敢不惜代价堆算力大规模计算，于是用户体验从难以接受到偶有惊喜
- 目前语言大模型说的话要么不太对，要么就是车轱辘话，还是垃圾营销号水平.
- 其实图像模型也是这个水平，不过图像一般人没有更高级的鉴赏能力，对于画作中的错误没办法看出来，所以图片领域更容易让用户吃惊

## [潘一鸣: AIGC 距离 iPhone 时刻还有多久](https://mp.weixin.qq.com/s/iDM7DkhTytOsnGLWLV9Xtg)

- 核心观点
- "成吉思汗的骑兵，攻击速度与 20 世纪的装甲部队相当；北宋的床弩，射程达一千五百米，与 20 世纪的狙击步枪差不多；但这些仍不过是古代的骑兵与弓弩而已，不可能与现代力量抗衡. "
- 由于基础理论(人工智能/认知科学)没有突破, 所以现在的 ai 无法做到"理解", 也就无法保证正确
- chatGPT, 类似于中文屋子思维模型, 屋内的工作人员并不了解中文, 他的回复依靠的是训练得到的规则手册, 而规则手册是黑箱.
- chatGPT 只是对文字按照规律进行处理, 本身并不知道自己在说什么
- 成本方面
  - ChatGPT 这样级别的预训练大模型跑一次的成本是千万美元级别，所以上线以来没有动过.
  - ChatGPT 单轮对话成本为几美分，大约十句话一块钱
  - 将其商业化部署，落地到类似聊天机器人这样多用户高并发任务中，成本只会更高
  - 因此, 受成本限制, bing-chatGPT 技术不会免费对外开放提供搜索服务, 必然是收费运行

## [#1 聊聊 ChatGPT](https://www.ateasthillside.com/p/1-chatgpt)

- 核心观点
- 指出依据 chatGPT 自身的介绍, 它是通过深度学习技术训练出的大型语言模型, 而非通用人工智能模型
- 对于语言模型而言, 其擅长方面为自然语言处理任务(聊天/写文件, 处理模型), 而非其他(例如数学).
- 只能处理文本内容, 无法处理其他任务(逻辑推理/语音识别/计算机视觉/...)
- 例证
  - 语言模型 chatGPT, 认为他有思考能力只是一种美丽的误解
  - [语言模型 chatGPT, 认为他有思考能力只是一种美丽的误解](https://mirror-4-web.bookflaneur.cn/https://tva1.sinaimg.cn/large/007Yq4pTly1hb152iis8wj31zm1ak7wh.jpg)

# 原理解析/论文综述

## [LLM 是什么](https://www.mittrchina.com/news/detail/10993)

- 核心观点
- LLM 指 大型语言模型 （LLMs，Large Language Models)
- 经过训练的 LLM 已经可以做到按照提示要求生成复杂的文本、回答较难理解的问题，甚至可以就某个主题展开对话
- LLM 的能力之所以如此优异，正是在于这些模型在训练的过程中，从由网络提取的大型文本语料库中吸取了数据量庞大的信息
- 但, LLM 并不能以此为基础, 进行语言类的处理工作. 原因在于 LLM 并不是以物理世界为基础的, 它的工作过程中无法获取周围环境信息与上下文, 这就导致 LLM 给出的不分答案显得不切实际

## [大型语言模型系列解读（一)：大语言模型涌现的新能力](https://zhuanlan.zhihu.com/p/601360789)

- 核心观点
- 语言模型是根据已知文本生成未知文本的模型
- prompting 相关的能力是随着模型规模的增大而涌现的
  - prompt => 提示

## [大型语言模型系列解读（二)：Transformer 中 FFN 的记忆功能](https://zhuanlan.zhihu.com/p/604739354)

- 核心观点
- 大型语言模型的强大能力离不开其对知识的记忆, 由于 LLM 本身不连接数据库, 所以其知识需要储存在模型的参数中.
- 这导致参数数量决定了模型掌握的知识量, 所以参数数量足够大才能表现出智能特征
- 知识存储于 FFN(前馈神经网络, 忽略该名词) 中, 内部仍然是基于统计规律/神经元网络决定具体输出结果

## [拆解追溯 GPT-3.5 各项能力的起源](https://yaofu.notion.site/GPT-3-5-360081d91ec245f29029d37b54573756#cf00f4e11d974187956122ce7d534386)

- 核心观点
- chatGPT 能力发展过程
  - GPT-3
    - 语言生成：遵循提示词（prompt)，然后生成补全提示词的句子 (completion).
    - 上下文学习 (in-context learning): 遵循给定任务的几个示例，然后为新的测试用例生成解决方案
    - 世界知识 (world knowledge)：包括事实性知识 (factual knowledge) 和常识 (commonsense).
    - 能力来源
      - 在有 3000 亿单词的语料上预训练拥有 1750 亿参数的模型（ 训练语料的 60%来自于 2016 - 2019 的 C4 + 22% 来自于 WebText2 + 16% 来自于 Books + 3%来自于 Wikipedia)
        - 参数数量决定知识量, 知识量爆炸产生涌现
      - 语言生成的能力来自于语言建模的训练目标 (language modeling)
      - 世界知识来自 3000 亿单词的训练语料库（不然还能是哪儿呢).
      - 模型的 1750 亿参数是为了存储知识，Liang et al. (2022) 的文章进一步证明了这一点. 他们的结论是，[知识密集型任务的性能与模型大小息息相关](https://crfm.stanford.edu/helm/v1.0/?group=knowledge)
      - 上下文学习的能力来源及为什么上下文学习可以泛化，**仍然难以溯源**
  - 改进手段:指令微调(不了解具体细节, 姑且任之)
    - **不会为模型注入新的能力** —— 所有的能力都已经存在了. 指令微调的作用是**解锁 / 激发这些能力**（基础的能力是通过预训练注入的)
    - 指令微调**将 GPT-3.5 的分化到不同的技能树**, 有些更擅长上下文学习，如`text-davinci-003`，有些更擅长对话，如`ChatGPT`
    - 指令微调通过**牺牲性能换取与人类的对齐（alignment)**, `code-davinci-002`在基准测试中实现了最佳性能（但模型不一定符合人类期望). 在`code-davinci-002`上进行指令微调后，模型可以生成更加符合人类期待的反馈（或者说模型与人类对齐)，例如：零样本问答、生成安全和公正的对话回复、拒绝超出模型它知识范围的问题.
  - `code-davinci-002`和`text-davinci-002`
    - 第一版的 GPT3.5 模型，一个用于代码，另一个用于文本
    - 响应人类指令：以前，GPT-3 的输出主要训练集中常见的句子. 现在的模型会针对指令 / 提示词生成更合理的答案（而不是相关但无用的句子).
    - 泛化到没有见过的任务：当用于调整模型的指令数量超过一定的规模时，模型就可以自动在从没见过的新指令上也能生成有效的回答. **这种能力对于上线部署至关重要，因为用户总会提新的问题，模型得答得出来才行. **
    - 代码生成和代码理解：这个能力很显然，因为模型用代码训练过.
    - **利用思维链 (chain-of-thought) 进行复杂推理**：初代 GPT3 的模型思维链推理的能力很弱甚至没有. **code-davinci-002 和 text-davinci-002 是两个拥有足够强的思维链推理能力的模型. **
      - 思维链: 在写 prompt 的时候，不仅给出结果，还要一步一步地写结果是怎么推出来的
      - 思维链推理之所以重要，是因为思维链可能是解锁突现能力和超越缩放法则 (scaling laws) 的关键
  - 新增能力的来源
    - 与之前模型相比, 新模型经历的主要区别是**指令微调**和**代码训练**
    - 响应人类指令
      - 指令微调的直接产物
    - **对没有见过的指令做出反馈的泛化能力**是在指令数量超过一定程度之后**自动出现**的(另一种涌现)
    - 使用**思维链**进行**复杂推理**的能力很可能是**代码训练的一个神奇的副产物**, 目前尚无定论, 但现有事实在暗示这一点
    - **代码训练**另一个可能的副产品是**长距离依赖**
      - “语言中的下个词语预测通常是非常局部的，而代码通常需要更长的依赖关系来做一些事情，比如前后括号的匹配或引用远处的函数定义”. 这里我想进一步补充的是：由于面向对象编程中的类继承，代码也可能有助于模型建立编码层次结构的能力. 我们将对这一假设的检验留给未来的工作.
  - ChatGPT: 基于人类反馈的强化学习(Reinforcement Learning from Human Feedback, RLHF) 的威力
    - 翔实的回应： text-davinci-003 的生成通常比 text-davinci-002 长. ChatGPT 的回应则更加冗长，以至于用户必须明确要求“用一句话回答我”，才能得到更加简洁的回答. 这是 RLHF 的直接产物
    - 公正的回应
    - 拒绝不当问题
    - 拒绝其知识范围之外的问题：例如，拒绝在 2021 年 6 月之后发生的新事件（因为它没在这之后的数据上训练过). 这是 RLHF 最神奇的部分，因为它使模型能够隐式地区分哪些问题在其知识范围内，哪些问题不在其知识范围内.
  - 需要注意的点
    - 所有的能力都是模型本来就有的， 而**不是通过 RLHF 注入的**. RLHF 的作用是**触发 / 解锁突现能力**
      - 这个论点主要来自于数据量大小的比较：因为与预训练的数据量相比，RLHF 占用的计算量 / 数据量要少得多.
    - 模型**知道它不知道什么不是通过编写规则来实现的**， 而是通过 RLHF 解锁的.
      - 这是一个非常令人惊讶的发现，因为 RLHF 的最初目标是让模型生成符合人类期望的回答，这更多是让模型生成安全的句子，而不是让模型知道它不知道的内容.
- 最终结论：
  - 语言生成能力 + 基础世界知识 + 上下文学习都是来自于预训练（`davinci`)
  - 存储大量知识的能力来自 1750 亿的参数量.
  - 遵循指令和泛化到新任务的能力来自于扩大指令学习中指令的数量（`Davinci-instruct-beta`)
  - 执行复杂推理的能力很可能来自于代码训练（`code-davinci-002`)
  - 生成中立、客观的能力、安全和翔实的答案来自与人类的对齐. 具体来说：
    - 如果是监督学习版，得到的模型是`text-davinci-002`
    - 如果是强化学习版 (RLHF) ，得到的模型是`text-davinci-003`
    - 无论是有监督还是 RLHF ，模型在很多任务的性能都无法超过 code-davinci-002 ，这种因为对齐而造成性能衰退的现象叫做对齐税.
  - 对话能力也来自于 RLHF（`ChatGPT`)，具体来说它牺牲了上下文学习的能力，来换取：
    - 建模对话历史
    - 增加对话信息量
    - 拒绝模型知识范围之外的问题
- GPT-3.5 目前不能做什么
  - 实时改写模型的信念：当模型表达对某事的信念时，如果该信念是错误的，我们可能很难纠正它
  - 形式推理：GPT-3.5 系列不能在数学或一阶逻辑等形式严格的系统中进行推理
    - 对于一些问题 (a) 非常模棱两可，没有推理；(b) 有点儿逻辑在里面，但有些地方也可以模糊；(c) 非常严谨，不能有任何歧义. 那么
    - 模型可以很好地进行 (b) 类的带模糊性的推理
      - 例如:
        - 生成如何做豆腐脑的方法. 做豆腐脑的时候，中间很多步骤模糊一点是可以接受的，比如到底是做咸的还是做甜的. 只要整体步骤大致正确，做出来的豆腐脑儿就能吃.
        - 数学定理的证明思路. 证明思路是用语言表达的非正式的逐步解法，其中每一步的严格推导可以不用太具体. 证明思路经常被用到数学教学：只要老师给一个大致正确的整体步骤，学生就可以大概明白. 然后老师把具体的证明细节作为作业布置给学生，答案略.
    - GPT-3.5 不能进行类型 (c) 的推理（推理不能容忍歧义)
  - 从互联网进行检索：GPT-3.5 系列（暂时)不能直接搜索互联网
- 结论:
  - 在这篇博文中，我们仔细检查了 GPT-3.5 系列的能力范围，并追溯了它们所有突现能力的来源.
  - 初代 GPT-3 模型通过预训练获得生成能力、世界知识和上下文学习(in-context learning, 在 prompt 里面写几个例子，模型就可以照着这些例子做生成).
  - 然后通过指令微调(instruction tuning, 用 instruction[指示] 来 fine-tune 大模型)的模型分支获得了遵循指令和能泛化到没有见过的任务的能力. 经过代码训练的分支模型则获得了代码理解的能力，作为代码训练的副产品，模型同时潜在地获得了复杂推理的能力.
  - 结合这两个分支，code-davinci-002 似乎是具有所有强大能力的最强 GPT-3.5 模型.
  - 接下来通过有监督的指令微调(instruction tuning)和 基于人类反馈的强化学习(RLHF)通过牺牲模型能力换取与人类对齐，即对齐税.
  - 基于人类反馈的强化学习(RLHF)使模型能够生成更翔实和公正的答案，同时拒绝其知识范围之外的问题.

## [深入理解语言模型的突现能力](https://yaofu.notion.site/514f4e63918749398a1a8a4c660e0d5b)

- 关注以下能力:
  1. NLP 社区近几年都关注，但之前的 NLP 模型很难达到的能力
  2. 源自于人类语言最深层的本质的能力（能力的深度)
  3. 可能达到人类智力的最高水平的能力（能力的上限)
- 模型应该多大才足够
  - 两个数字：62B 和 175B.
    - 模型至少需要 62B，使思维链的效果才能大于标准的提示词方法.
      - 62B 这个数字来自于 Chung 等人 2022 年工作的第五张表：
      - 对于所有小于 62B 的模型，直接用提示词都好于思维链
    - 模型至少需要 175B（GPT3 的尺寸)，思维链的效果才能大于精调小模型（T5 11B)的效果.

# 原理解析/科普

以下两篇文章均是解释了神经网络的统计学模型本质. 第一篇作者是`Stephen Wolfram`, 非常详尽但不一定通俗. 第二篇是简单科普, 可以作为入门平替

## [万字长文解释 ChatGPT 在做什么，以及为什么它能发挥作用](https://mp.weixin.qq.com/s/ynP5gBSv4nQXpX8vEChUUg)

## [ChatGPT 背后的 OpenAI 是家怎样的公司](https://www.zhihu.com/question/583348944/answer/2891376088)
